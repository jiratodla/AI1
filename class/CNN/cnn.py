import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, MaxPool2D, Flatten, Dense, Rescaling, Dropout
import pathlib
import os

# 1. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏ü‡∏•‡πå
data_dir_path = r"C:\Users\CPE-CDTI_X390\Desktop\AI\class\CNN\dataset"
data_dir = pathlib.Path(data_dir_path)

# --- [‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏° 1] ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏á‡πÑ‡∏´‡∏° ---
if not data_dir.exists():
    print(f"‚ùå Error: ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà {data_dir_path}")
    print("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏ä‡πá‡∏Ñ path ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÑ‡∏ß‡πâ‡∏ñ‡∏π‡∏Å‡∏ó‡∏µ‡πà‡πÑ‡∏´‡∏°")
    exit() # ‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
img_height = 64
img_width = 64
batch_size = 32

print(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å: {data_dir}")

# 2. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Load Data)
# ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≠‡∏¢‡∏°‡∏≤‡∏Å (‡πÄ‡∏ä‡πà‡∏ô‡∏°‡∏µ‡πÅ‡∏Ñ‡πà 2-10 ‡∏£‡∏π‡∏õ) ‡∏ú‡∏°‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏õ‡∏¥‡∏î validation_split ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
# ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏£‡∏π‡∏õ‡πÄ‡∏¢‡∏≠‡∏∞ (‡∏´‡∏•‡∏±‡∏Å‡∏£‡πâ‡∏≠‡∏¢) ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡∏¥‡∏î validation_split=0.2 ‡πÑ‡∏ß‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°

try:
    train_ds = tf.keras.utils.image_dataset_from_directory(
      data_dir,
      validation_split=0.2,
      subset="training",
      seed=123,
      image_size=(img_height, img_width),
      batch_size=batch_size
    )

    val_ds = tf.keras.utils.image_dataset_from_directory(
      data_dir,
      validation_split=0.2,
      subset="validation",
      seed=123,
      image_size=(img_height, img_width),
      batch_size=batch_size
    )
except ValueError as e:
    print("\n‚ö†Ô∏è ‡πÄ‡∏Å‡∏¥‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ï‡∏≠‡∏ô‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•!")
    print("‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ: ‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏£‡∏π‡∏õ‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ‡∏à‡∏ô‡πÅ‡∏ö‡πà‡∏á 80/20 ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡∏´‡∏£‡∏∑‡∏≠ ‡∏Ñ‡∏∏‡∏ì‡∏•‡∏∑‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏¢‡πà‡∏≠‡∏¢ (Class)")
    print(f"Error details: {e}")
    exit()

class_names = train_ds.class_names
num_classes = len(class_names)
print(f"\n‚úÖ ‡πÄ‡∏à‡∏≠‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {num_classes} ‡∏Ñ‡∏•‡∏≤‡∏™ ‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà: {class_names}")

# 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
model = Sequential([
  Input(shape=(img_height, img_width, 3)),
  Rescaling(1./255),
  
  # Layer 1
  Conv2D(32, 3, padding='same', activation='relu'),
  BatchNormalization(),
  MaxPool2D(),
  
  # Layer 2
  Conv2D(64, 3, padding='same', activation='relu'),
  BatchNormalization(),
  MaxPool2D(),
  
  # Layer 3 (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏™‡πâ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏á‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô)
  Conv2D(128, 3, padding='same', activation='relu'),
  BatchNormalization(),
  MaxPool2D(),
  
  Flatten(),
  
  # --- [‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏° 2] ‡πÉ‡∏™‡πà Dropout ---
  Dense(128, activation='relu'),
  Dropout(0.5), # ‡∏™‡∏∏‡πà‡∏°‡∏•‡∏∑‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 50% ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≠‡∏¢)
  
  Dense(num_classes, activation='softmax')
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
              metrics=['accuracy'])

model.summary()

# 4. ‡∏™‡∏±‡πà‡∏á‡πÄ‡∏ó‡∏£‡∏ô
print("\nüöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•...")
# ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≠‡∏¢ ‡πÉ‡∏´‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏° epochs ‡πÄ‡∏¢‡∏≠‡∏∞‡∏´‡∏ô‡πà‡∏≠‡∏¢ (‡πÄ‡∏ä‡πà‡∏ô 10-20 ‡∏£‡∏≠‡∏ö)
epochs = 10 
history = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs
)

print("\nüéâ ‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢!")